{
    "metadata": {
        "experiment_name": "Ramedis Baseline con GPT-4o",
        "llm_configs": {
            "candidate_dx_gpt": {
                "model": "o1",
                "prompt": "eval-prompts/candidate_prompt.txt"
            },
            "severity_assigner_llm": {
                "model": "gpt-4o",
                "prompt": "eval-prompts/severity_assignment_batch_prompt.txt"
            }
        }
    },
    "semantic_evaluation": {
        "mean_score": 0.8021,
        "standard_deviation": 0.204,
        "range": {
            "min": 0.3066,
            "max": 1.0
        }
    },
    "severity_evaluation": {
        "mean_score": 0.2196,
        "standard_deviation": 0.1005,
        "range": {
            "min": 0.0,
            "max": 0.625
        }
    }
}