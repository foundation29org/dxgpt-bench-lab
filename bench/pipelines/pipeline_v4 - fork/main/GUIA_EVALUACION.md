# Gu√≠a de Evaluaci√≥n - Pipeline V4

Esta gu√≠a explica paso a paso c√≥mo ejecutar la evaluaci√≥n completa de diagn√≥sticos diferenciales usando el Pipeline V4.

## üìã Tabla de Contenidos

1. [Prerequisitos](#prerequisitos)
2. [Configuraci√≥n Inicial](#configuraci√≥n-inicial)
3. [Pasos de la Evaluaci√≥n](#pasos-de-la-evaluaci√≥n)
4. [Configuraci√≥n del Pipeline](#configuraci√≥n-del-pipeline)
5. [Ejecuci√≥n](#ejecuci√≥n)
6. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
7. [Troubleshooting](#troubleshooting)

---

## Prerequisitos

### 1. Variables de Entorno

Crea un archivo `.env` en la **ra√≠z del proyecto** (`C:\repo\DxGPT\eval\.env`) con las siguientes variables:

```env
# Azure Text Analytics (para atribuci√≥n de c√≥digos m√©dicos)
AZURE_LANGUAGE_ENDPOINT=https://tu-endpoint.cognitiveservices.azure.com
AZURE_LANGUAGE_KEY=tu_clave_azure

# Azure OpenAI (para modelos LLM)
AZURE_OPENAI_ENDPOINT=https://tu-endpoint.openai.azure.com
AZURE_OPENAI_API_KEY=tu_clave_azure_openai
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Google Gemini (para modelos Gemini)
GOOGLE_GENAI_API_KEY=tu_clave_gemini
# O alternativamente:
GEMINI_API_KEY=tu_clave_gemini

# Azure Translator (opcional, para traducci√≥n de casos)
AZURE_TRANSLATOR_KEY=tu_clave_translator
AZURE_TRANSLATOR_ENDPOINT=https://api.cognitive.microsofttranslator.com
AZURE_TRANSLATOR_REGION=tu_region

# Hugging Face (para BERT similarity)
HF_TOKEN=tu_token_huggingface
SAPBERT_ENDPOINT_URL=tu_endpoint_sapbert
```

### 2. Dependencias Python

Instala las dependencias necesarias:

```bash
pip install -r requirements.txt
```

O usando `uv`:

```bash
uv pip install -r requirements.txt
```

### 3. Dataset

Aseg√∫rate de tener el dataset en la ruta especificada en `config.yaml`. Por defecto:
- `bench/datasets/all_150.json`

---

## Configuraci√≥n Inicial

### 1. Navegar al Directorio del Pipeline

```bash
cd "bench/pipelines/pipeline_v4 - fork/main"
```

### 2. Validar Configuraci√≥n

Antes de ejecutar, valida que todo est√© correcto:

```bash
py validate.py
```

Esto verificar√°:
- ‚úÖ Variables de entorno configuradas
- ‚úÖ Archivos de configuraci√≥n v√°lidos
- ‚úÖ Dataset accesible
- ‚úÖ Dependencias instaladas

---

## Pasos de la Evaluaci√≥n

El pipeline ejecuta **3 pasos principales** en secuencia:

### **PASO 1: Emulator (Generaci√≥n de DDX)**

**¬øQu√© hace?**
- Lee cada caso del dataset
- Env√≠a el caso al modelo LLM configurado (GPT-5, Gemini, etc.)
- El LLM genera diagn√≥sticos diferenciales (DDX) basados en el caso cl√≠nico
- Opcionalmente traduce el caso si `TRANSLATE_CASE.ENABLED: true`

**Entrada:**
- Dataset JSON con casos cl√≠nicos (`all_150.json`)

**Salida:**
- Archivo JSON con DDX generados por el LLM
- Ubicaci√≥n: `output/<dataset>/<prompt>/<model>/ddxs_from_emulator.json`

**Tiempo estimado:**
- Depende del modelo y n√∫mero de casos
- Gemini 2.5 Pro: ~75 segundos para 150 casos (con tier 1)
- GPT-5.1: ~13-14 segundos por caso

---

### **PASO 2: Medical Labeler (Atribuci√≥n de C√≥digos M√©dicos)**

**¬øQu√© hace?**
- Toma los DDX generados en el Paso 1
- Para cada diagn√≥stico diferencial, llama a Azure Text Analytics
- Azure identifica entidades m√©dicas y extrae c√≥digos:
  - **ICD-10**: C√≥digos de clasificaci√≥n internacional
  - **SNOMED**: Terminolog√≠a cl√≠nica estandarizada
  - **OMIM**: Base de datos de genes y fenotipos
  - **ORPHA**: Enfermedades raras

**Entrada:**
- Archivo JSON con DDX del Paso 1

**Salida:**
- Archivo JSON con DDX + c√≥digos m√©dicos asignados
- Ubicaci√≥n: `output/<dataset>/<prompt>/<model>/ddxs_from_labeler.json`

**Tiempo estimado:**
- ~5-10 minutos para 150 casos (depende de Azure Text Analytics)

**Nota:** El archivo del emulator se elimina autom√°ticamente despu√©s de este paso.

---

### **PASO 3: Evaluator (Evaluaci√≥n de Calidad)**

**¬øQu√© hace?**
- Compara los DDX generados con los diagn√≥sticos de referencia (GDX)
- Usa **3 m√©todos de evaluaci√≥n** en orden de prioridad:

  1. **SNOMED Match**: Coincidencia exacta de c√≥digos SNOMED
  2. **ICD-10 Match**: Coincidencia de c√≥digos ICD-10 (exacta, padre, hijo, hermano)
  3. **Semantic Match**: Similaridad sem√°ntica usando BERT + juicio de LLM

**Criterios de aceptaci√≥n:**
- **BERT_AUTOCONFIRM_THRESHOLD** (0.90): Si BERT score ‚â• 0.90, acepta autom√°ticamente (no llama al LLM)
- **BERT_ACCEPTANCE_THRESHOLD** (0.80): Si BERT score ‚â• 0.80, requiere confirmaci√≥n del LLM juez
- Si BERT < 0.80, el **LLM juez** decide si hay match (ver secci√≥n [Modelo Juez](#modelo-juez-judge_model) m√°s abajo)

**Entrada:**
- Archivo JSON con DDX + c√≥digos del Paso 2
- Dataset original con GDX (diagn√≥sticos de referencia)

**Salida:**
- `evaluation.log`: Log detallado del proceso
- `evaluation_details.txt`: Detalles de cada caso evaluado
- `summary.json`: Resumen estad√≠stico de resultados
- Ubicaci√≥n: `output/<dataset>/<prompt>/<model>/<timestamp>/`

**Tiempo estimado:**
- ~5-10 minutos para 150 casos

---

## Configuraci√≥n del Pipeline

Edita el archivo `config.yaml` para personalizar la evaluaci√≥n:

### Modelo LLM

```yaml
DXGPT_EMULATOR:
  MODEL: "gemini-2.5-pro"  # Opciones: "gpt-5.1", "gpt-5-mini", "gemini-2.5-pro", "gemini-2.0-flash", etc.
```

**Modelos disponibles:**
- **Azure OpenAI**: `gpt-5.1`, `gpt-5-mini`, `gpt-4o-summary`, `o3-mini`
- **Google Gemini**: `gemini-2.5-pro`, `gemini-2.0-flash`, `gemini-3-pro-preview`

### Par√°metros del Modelo

```yaml
PARAMS:
  temperature: 0.1          # Creatividad (0.0 = determinista, 1.0 = creativo)
  max_tokens: 12000         # M√°ximo de tokens en la respuesta
  reasoning_effort: "low"   # Para O3/GPT-5: "low", "medium", "high"
  thinking_level: "low"     # Para Gemini 2.5/3: "low", "medium", "high"
```

### Traducci√≥n de Casos

```yaml
TRANSLATE_CASE:
  ENABLED: true             # Activar/desactivar traducci√≥n
  TARGET_LANGUAGE: "en"     # Idioma objetivo ("en" = ingl√©s, "es" = espa√±ol)
```

**¬øCu√°ndo usar traducci√≥n?**
- Si tu dataset tiene casos en espa√±ol y quieres evaluar el modelo en ingl√©s
- Puede mejorar el rendimiento si el modelo est√° mejor entrenado en ingl√©s

### Thresholds de Evaluaci√≥n

```yaml
EVALUATOR:
  BERT_ACCEPTANCE_THRESHOLD: 0.80   # Score m√≠nimo para considerar match (0.0-1.0)
  BERT_AUTOCONFIRM_THRESHOLD: 0.90 # Score para aceptar autom√°ticamente (0.0-1.0)
  ENABLE_ICD10_PARENT_SEARCH: true  # Buscar c√≥digos padre en ICD-10
  ENABLE_ICD10_SIBLING_SEARCH: true # Buscar c√≥digos hermanos en ICD-10
  JUDGE_MODEL: null  # Modelo LLM para juzgar matches sem√°nticos (opcional)
```

**Recomendaciones:**
- **BERT_ACCEPTANCE_THRESHOLD**: 0.80 es conservador, 0.70 es m√°s permisivo
- **BERT_AUTOCONFIRM_THRESHOLD**: 0.90 es est√°ndar, no cambiar a menos que haya problemas
- **JUDGE_MODEL**: Modelo usado para juzgar si hay match sem√°ntico cuando BERT < 0.80

### Modelo Juez (JUDGE_MODEL)

El modelo juez es el LLM que decide si hay match sem√°ntico cuando el score BERT est√° por debajo del threshold. Por defecto, usa **l√≥gica autom√°tica**:

**Comportamiento autom√°tico (JUDGE_MODEL: null):**
- Si eval√∫as un modelo **Gemini** (ej: `gemini-2.5-pro`) ‚Üí el juez ser√° el **mismo modelo Gemini**
- Si eval√∫as un modelo **OpenAI** (ej: `gpt-5.1`, `gpt-5-mini`, `o3-mini`) ‚Üí el juez ser√° **`gpt-4o-summary`**

**Configuraci√≥n manual:**
```yaml
EVALUATOR:
  JUDGE_MODEL: "gemini-2.5-pro"  # Fuerza usar este modelo siempre
  # O
  JUDGE_MODEL: "gpt-4o-summary"  # Fuerza usar GPT-4o-summary siempre
```

**¬øPor qu√© usar el mismo modelo?**
- **Consistencia**: Si eval√∫as Gemini, el juez tambi√©n es Gemini (m√°s preciso)
- **Precisi√≥n**: El mismo modelo juzga sus propias respuestas
- **Neutralidad**: Para OpenAI, usar `gpt-4o-summary` evita sesgo del modelo evaluado

**Ejemplo:**
- Eval√∫as `gemini-2.5-pro` ‚Üí Juez autom√°tico: `gemini-2.5-pro`
- Eval√∫as `gpt-5-mini` ‚Üí Juez autom√°tico: `gpt-4o-summary`
- Eval√∫as `gemini-2.0-flash` ‚Üí Juez autom√°tico: `gemini-2.0-flash`

### Control del Pipeline

```yaml
MAIN:
  SHOULD_EMULATE: true   # Paso 1: Generar DDX usando el modelo LLM
  SHOULD_LABEL: true     # Paso 2: Asignar c√≥digos m√©dicos usando Azure Text Analytics
  SHOULD_EVALUATE: true  # Paso 3: Evaluar DDX contra GDX
```

**Casos de uso comunes:**

| Escenario | SHOULD_EMULATE | SHOULD_LABEL | SHOULD_EVALUATE | Descripci√≥n |
|-----------|----------------|--------------|-----------------|-------------|
| **Ejecuci√≥n completa desde cero** | `true` | `true` | `true` | Ejecuta todos los pasos (o omite estos valores) |
| **Solo re-evaluar** | `false` | `false` | `true` | Usa DDX y c√≥digos existentes, solo re-eval√∫a |
| **Solo generar DDX** | `true` | `false` | `false` | Genera diagn√≥sticos pero no asigna c√≥digos ni eval√∫a |
| **Generar DDX + c√≥digos** | `true` | `true` | `false` | Genera DDX y asigna c√≥digos, sin evaluar |
| **Solo asignar c√≥digos** | `false` | `true` | `false` | Si ya tienes DDX, solo asigna c√≥digos m√©dicos |

**Nota:** Si omites estos valores, el pipeline asume `true` por defecto (ejecuta todos los pasos).

---

## Ejecuci√≥n

### Ejecuci√≥n Completa

```bash
# Desde el directorio del pipeline
cd "bench/pipelines/pipeline_v4 - fork/main"

# Ejecutar pipeline completo
py main.py
```

### Ejecuci√≥n por Pasos

Si quieres ejecutar pasos individuales:

```bash
# Solo Paso 1: Generar DDX
py emulator.py

# Solo Paso 2: Asignar c√≥digos m√©dicos
py medlabeler.py

# Solo Paso 3: Evaluar
py evaluator.py
```

### Gesti√≥n de Estado

El pipeline detecta autom√°ticamente si ya existen resultados:

**Si existen DDX del Paso 1:**
```
‚ö†Ô∏è  DDX results already exist at output/...

1. Re-run DDX generation (will overwrite existing results)
2. Continue with medical code labeling using existing DDX
3. ‚ùå Abort operation

Enter your choice (number):
```

**Si existen c√≥digos del Paso 2:**
```
‚ö†Ô∏è  Labeled results already exist at output/...

1. Re-run medical code labeling (will overwrite existing results)
2. Continue with evaluation using existing labeled results
3. ‚ùå Abort operation

Enter your choice (number):
```

---

## Interpretaci√≥n de Resultados

### Archivo `summary.json`

Contiene las m√©tricas principales:

```json
{
  "total_cases": 150,
  "successful_matches": 52,
  "success_rate": 0.347,
  "ddx_positions": {
    "P1": 42,  // Diagn√≥stico correcto en posici√≥n 1
    "P2": 8,   // Diagn√≥stico correcto en posici√≥n 2
    "P3": 1,   // Diagn√≥stico correcto en posici√≥n 3
    "P4": 1    // Diagn√≥stico correcto en posici√≥n 4
  },
  "average_position": 1.250
}
```

**M√©tricas clave:**
- **success_rate**: Porcentaje de casos donde se encontr√≥ el diagn√≥stico correcto
- **P1, P2, P3, P4**: Distribuci√≥n de posiciones donde se encontr√≥ el diagn√≥stico
- **average_position**: Posici√≥n promedio del diagn√≥stico correcto (menor es mejor)

### Archivo `evaluation.log`

Log detallado con el resultado de cada caso:

```
[1/150] BERT_AUTOCONFIRM ‚Üí GDX[1]: Systemic lupus erythematosus | DDX[1]: Systemic Lupus Erythematosus ‚Üí **P1**
[2/150] SEMANTIC ‚Üí NO_MATCH ‚Üí **REJECTED**
[3/150] BERT_AUTOCONFIRM ‚Üí GDX[1]: POEMS syndrome | DDX[2]: POEMS Syndrome ‚Üí **P2**
```

**C√≥digos de resultado:**
- `BERT_AUTOCONFIRM`: Match encontrado con score BERT ‚â• 0.90
- `BERT_ACCEPTANCE`: Match encontrado con score BERT ‚â• 0.80 (confirmado por LLM)
- `SNOMED`: Match encontrado por c√≥digo SNOMED
- `ICD10_EXACT`: Match encontrado por c√≥digo ICD-10 exacto
- `ICD10_PARENT`: Match encontrado por c√≥digo ICD-10 padre
- `ICD10_SIBLING`: Match encontrado por c√≥digo ICD-10 hermano
- `SEMANTIC`: Match encontrado por similaridad sem√°ntica (LLM)
- `NO_MATCH` / `REJECTED`: No se encontr√≥ match

### Archivo `evaluation_details.txt`

Contiene informaci√≥n detallada de cada caso, incluyendo:
- GDX evaluado (diagn√≥stico de referencia)
- DDX generados (diagn√≥sticos diferenciales)
- Scores BERT para cada DDX
- Traza completa de la evaluaci√≥n (SNOMED ‚Üí ICD-10 ‚Üí Semantic)

---

## Troubleshooting

### Error: "Azure Language service credentials not found"

**Soluci√≥n:**
- Verifica que el archivo `.env` est√© en la ra√≠z del proyecto (`C:\repo\DxGPT\eval\.env`)
- Verifica que las variables `AZURE_LANGUAGE_ENDPOINT` y `AZURE_LANGUAGE_KEY` est√©n configuradas

### Error: "Module not found"

**Soluci√≥n:**
```bash
pip install -r requirements.txt
```

### Error: "429 RESOURCE_EXHAUSTED" (Gemini)

**Causa:** L√≠mite de rate limit excedido

**Soluci√≥n:**
- El c√≥digo ya incluye delays autom√°ticos seg√∫n el modelo
- Para tier gratuito, los delays son m√°s largos (30s para 2.5-pro)
- Con facturaci√≥n (tier 1), los delays son m√°s cortos (0.5s para 2.5-pro)
- Si persiste, aumenta el delay en `emulator.py`

### Muchos casos "REJECTED"

**Posibles causas:**
1. **Medlabeler no asign√≥ c√≥digos**: Revisa `medlabeler.log` para ver si Azure devolvi√≥ c√≥digos
2. **Threshold de BERT demasiado estricto**: Considera bajar `BERT_ACCEPTANCE_THRESHOLD` a 0.70
3. **LLM demasiado conservador**: El LLM puede estar rechazando matches v√°lidos

**Soluci√≥n:**
- Revisa `evaluation_details.txt` para casos espec√≠ficos
- Verifica los scores BERT en los casos rechazados
- Si los scores son altos (>0.65) pero no alcanzan el threshold, considera bajarlo

### El pipeline se detiene en medio de la ejecuci√≥n

**Soluci√≥n:**
- El pipeline guarda el estado autom√°ticamente
- Puedes reanudar desde donde se qued√≥ ejecutando `py main.py` de nuevo
- Elige la opci√≥n "Continue" cuando te pregunte sobre archivos existentes

### Resultados diferentes entre ejecuciones

**Causa:** Los modelos LLM pueden tener variabilidad incluso con `temperature: 0.1`

**Soluci√≥n:**
- Esto es normal, especialmente con modelos no deterministas
- Para comparaciones justas, usa el mismo `seed` si el modelo lo soporta
- Ejecuta m√∫ltiples veces y promedia los resultados

---

## Estructura de Archivos de Salida

```
output/
‚îî‚îÄ‚îÄ all_150/                          # Nombre del dataset
    ‚îî‚îÄ‚îÄ juanjo_classic_v2/            # Nombre del prompt
        ‚îî‚îÄ‚îÄ gemini_2_5_pro_low_translated_en/  # Nombre del modelo (con sufijos)
            ‚îú‚îÄ‚îÄ juanjo_classic_v2___gemini_2_5_pro_low_translated_en___ddxs_from_labeler.json
            ‚îú‚îÄ‚îÄ juanjo_classic_v2___gemini_2_5_pro_low_translated_en___config.yaml
            ‚îú‚îÄ‚îÄ emulator.log
            ‚îú‚îÄ‚îÄ medlabeler.log
            ‚îî‚îÄ‚îÄ 20251204151832/        # Timestamp de la evaluaci√≥n
                ‚îú‚îÄ‚îÄ juanjo_classic_v2___gemini_2_5_pro_low_translated_en___evaluation.log
                ‚îú‚îÄ‚îÄ juanjo_classic_v2___gemini_2_5_pro_low_translated_en___evaluation_details.txt
                ‚îú‚îÄ‚îÄ juanjo_classic_v2___gemini_2_5_pro_low_translated_en___summary.json
                ‚îî‚îÄ‚îÄ juanjo_classic_v2___gemini_2_5_pro_low_translated_en___config.yaml
```

**Nota sobre nombres de modelos:**
- Si `TRANSLATE_CASE.ENABLED: true`, se a√±ade el sufijo `_translated_<idioma>`
- Los espacios y caracteres especiales se reemplazan por `_`

---

## Consejos y Mejores Pr√°cticas

1. **Empieza con un dataset peque√±o**: Prueba con 5-10 casos antes de ejecutar 150
2. **Revisa los logs**: Los archivos `.log` contienen informaci√≥n valiosa sobre errores
3. **Compara modelos**: Ejecuta la misma configuraci√≥n con diferentes modelos para comparar
4. **Guarda configuraciones**: El pipeline guarda autom√°ticamente una copia del `config.yaml` en cada ejecuci√≥n
5. **Monitorea los rate limits**: Especialmente con modelos Gemini en tier gratuito
6. **Revisa casos espec√≠ficos**: Usa `evaluation_details.txt` para entender por qu√© un caso fue rechazado

---

## Siguiente Paso

Una vez completada la evaluaci√≥n, puedes:
- Comparar resultados entre diferentes modelos
- Analizar casos espec√≠ficos en `evaluation_details.txt`
- Ajustar thresholds si hay muchos falsos negativos/positivos
- Generar visualizaciones de los resultados

---

**√öltima actualizaci√≥n:** Diciembre 2024

