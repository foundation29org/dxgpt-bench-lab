# Pipeline v2 - ICD10 + BERT

## Descripci√≥n General

Pipeline de evaluaci√≥n m√©dica que combina diagn√≥sticos diferenciales generados por LLMs con codificaci√≥n m√©dica estandarizada (ICD10, SNOMED) y evaluaci√≥n sem√°ntica mediante BERT como fallback. Dise√±ado para evaluar la precisi√≥n diagn√≥stica y alineaci√≥n de severidad entre diagn√≥sticos generados (DDX) y diagn√≥sticos de referencia (GDX).

## üìä Origen de los Datos: El Ecosistema de Datasets

### Pool de Datos Total: 9,677 casos m√©dicos

El pipeline se alimenta de un conjunto diverso de fuentes m√©dicas, totalizando **9,677 casos** organizados en 7 datasets:

| Origen | Archivo | Casos | Descripci√≥n | Multi-Dx | Complejidad | Severidad |
|--------|---------|-------|-------------|----------|-------------|-----------|
| **R** | `ramedis.json` | 852 | Casos de enfermedades raras (Rare Disease Medical Information System) | 34.7% | C9-C10 | S9-S10 |
| **Q** | `medqausmle4op.json` | 7,075 | Preguntas m√©dicas del USMLE (United States Medical Licensing Examination) | 0% | C0-C9 | S0-S9 |
| **T** | `urgtorre.json` | 1,398 | Casos de urgencias hospitalarias | 37.5% | C1-C8 | S1-S9 |
| **S** | `rare_synthetic.json` | 200 | Casos sint√©ticos de enfermedades raras generados por IA | 0% | C2-C8 | S3-S9 |
| **B** | `medbulltes5op.json` | 141 | Casos m√©dicos generales | 0% | C0-C9 | S0-S9 |
| **J** | `new_england_med_journal.json` | 2 | Casos del New England Journal of Medicine | 50% | C5 | S5-S6 |
| **U** | `ukranian.json` | 9 | Casos m√©dicos de Ucrania | 77.8% | C2-C8 | S2-S8 |

### Estructura de los Datos

#### Estructura de un Caso M√©dico

**En datasets originales (post-normalization):**
```json
{
  "id": "R001",                    // Identificador √∫nico (letra origen + n√∫mero)
  "case": "Descripci√≥n cl√≠nica detallada del paciente...",
  "complexity": "C7",              // Complejidad del caso (C0-C10)
  "diagnoses": [                   // Lista de diagn√≥sticos correctos (GDX)
    {
      "name": "Ataxia-telangiectasia",
      "severity": "S8"             // Severidad (S0=m√≠nima, S10=cr√≠tica)
    }
  ]
}
```

**En datasets servidos (/served/):**
```json
{
  "id": "R001",
  "case": "Descripci√≥n cl√≠nica...",
  "complexity": "C7",
  "diagnoses": [
    {
      "name": "Ataxia-telangiectasia",
      "normalized_text": "Ataxia telangiectasia",  // Texto normalizado
      "severity": "S8",
      "medical_codes": {}          // Vac√≠o inicialmente, se llena durante evaluaci√≥n
    }
  ]
}
```

**Nota importante**: Los c√≥digos m√©dicos (ICD10, SNOMED, ORPHA, OMIM) NO est√°n presentes en los datasets base. Se extraen din√°micamente durante la evaluaci√≥n del pipeline usando Azure Text Analytics Health API.

#### Estad√≠sticas por Dataset

- **Diagn√≥sticos √∫nicos totales**: ~6,500 condiciones m√©dicas diferentes
- **Casos multi-diagn√≥stico**: 828 casos (8.6% del total)
  - M√°s comunes en urgencias (T: 37.5%) y casos ucranianos (U: 77.8%)
  - Ausentes en USMLE (Q) y datasets sint√©ticos (S, B)
- **Cobertura de c√≥digos m√©dicos**: Los datasets post-normalizaci√≥n NO incluyen c√≥digos m√©dicos. Estos se extraen din√°micamente durante la evaluaci√≥n usando Azure Text Analytics
- **Distribuci√≥n de complejidad**: Desde casos simples (C0) hasta altamente complejos (C10)
- **Distribuci√≥n de severidad**: Desde condiciones menores (S0) hasta cr√≠ticas (S10)

### üéØ Creaci√≥n de Datasets de Evaluaci√≥n con `serve.py`

Para crear datasets balanceados y diversos para evaluaci√≥n, se utiliza el script `serve.py` ubicado en `data29/data-repos/post-normalization (server)/`. Este script implementa un algoritmo inteligente de selecci√≥n que:

#### Algoritmo de Selecci√≥n Inteligente

1. **Sistema de Puntuaci√≥n**:
   - 5 puntos √ó cap√≠tulos ICD10 nuevos introducidos
   - 3 puntos √ó bonus por fuente subrepresentada
   - 2 puntos √ó (complejidad/10)
   - 1 punto √ó casos multi-diagn√≥stico
   - 0.5 puntos √ó (severidad/10)

2. **Fases del Algoritmo**:
   - **Fase 0**: Carga y preparaci√≥n de datos
   - **Fase 1**: Selecci√≥n prioritaria respetando reglas min/max por fuente
   - **Fase 2**: Llenado iterativo inteligente maximizando diversidad
   - **Fase 3**: Generaci√≥n de reportes y visualizaciones

#### Estructura de Salida en `/served/`

```
served/
‚îî‚îÄ‚îÄ YYYYMMDD_HHMMSS_cN/           # N = n√∫mero de casos seleccionados
    ‚îú‚îÄ‚îÄ aggregated_*.json          # Dataset final seleccionado
    ‚îú‚îÄ‚îÄ report_*.json              # Estad√≠sticas detalladas en JSON
    ‚îú‚îÄ‚îÄ report_*.txt               # Reporte legible para humanos
    ‚îî‚îÄ‚îÄ plots/                     # 9 visualizaciones sofisticadas
        ‚îú‚îÄ‚îÄ 1_source_composition.png
        ‚îú‚îÄ‚îÄ 2_icd10_chapters.png
        ‚îú‚îÄ‚îÄ 3_complexity_distribution.png
        ‚îú‚îÄ‚îÄ 4_severity_distribution.png
        ‚îú‚îÄ‚îÄ 5_diagnoses_per_case.png
        ‚îú‚îÄ‚îÄ 6_top_diagnoses.png
        ‚îú‚îÄ‚îÄ 7_source_to_icd10_flow.png
        ‚îú‚îÄ‚îÄ 8_complexity_severity_heatmap.png
        ‚îî‚îÄ‚îÄ 9_diagnosis_wordcloud.png
```

#### Ejemplo de Uso

```bash
# Crear dataset de 450 casos con configuraci√≥n personalizada
python serve.py --size 450 --config custom_rules.json
```

Los datasets generados garantizan:
- ‚úÖ Cobertura de los 22 cap√≠tulos ICD10 (A-Z)
- ‚úÖ Balance entre fuentes de datos
- ‚úÖ Diversidad diagn√≥stica (cientos de diagn√≥sticos √∫nicos)
- ‚úÖ Distribuci√≥n equilibrada de complejidad (C2-C10)
- ‚úÖ ~30% de casos multi-diagn√≥stico

#### üì¶ Flujo: served ‚Üí bench/datasets

La carpeta `/served/` act√∫a como √°rea de staging donde `serve.py` genera datasets amalgamados listos para evaluaci√≥n. El flujo t√≠pico es:

```
1. data29/data-repos/post-normalization (server)/
   ‚îî‚îÄ‚îÄ serve.py ‚Üí genera ‚Üí served/YYYYMMDD_HHMMSS_c450/aggregated_*.json
   
2. Usuario copia manualmente el dataset generado:
   served/aggregated_*.json ‚Üí bench/datasets/all_450.json
   
3. Pipeline v2 consume el dataset:
   config.yaml: dataset_path: "bench/datasets/all_450.json"
```

**Importante**: Los datasets en `/served/` son temporales y espec√≠ficos para cada experimento. Una vez validados, se copian a `bench/datasets/` con nombres descriptivos (ej: `all_450.json`, `rare_only_100.json`, `emergency_focus_250.json`) para su reutilizaci√≥n en m√∫ltiples experimentos.

## Estructura del Directorio del Pipeline

```
pipeline_v2 - icd10 + bert/
‚îú‚îÄ‚îÄ config.yaml                          # Configuraci√≥n principal del pipeline
‚îú‚îÄ‚îÄ run.py                              # Script principal de ejecuci√≥n
‚îú‚îÄ‚îÄ README.md                           # Esta documentaci√≥n
‚îú‚îÄ‚îÄ model_comparison_*.txt              # Reportes comparativos entre modelos
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison_20250703_021428_without_bert.txt  # Sin BERT fallback
‚îÇ   ‚îî‚îÄ‚îÄ model_comparison_20250704_140708_with_bert.txt     # Con BERT fallback
‚îú‚îÄ‚îÄ eval-prompts/                       # Prompts para evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ severity_assignment_batch_prompt.txt    # Prompt para asignar severidades
‚îÇ   ‚îî‚îÄ‚îÄ severity_assignment_batch_schema.json   # Schema de salida para severidades
‚îî‚îÄ‚îÄ results/                           # Resultados organizados por modelo
    ‚îú‚îÄ‚îÄ gpt_4_1/                       # Resultados de GPT-4.1
    ‚îú‚îÄ‚îÄ gpt_4o_summary/                # Resultados de GPT-4o con resumen
    ‚îú‚îÄ‚îÄ o1/                            # Resultados del modelo O1
    ‚îú‚îÄ‚îÄ o3_images/                     # Resultados de O3 con im√°genes
    ‚îî‚îÄ‚îÄ o3_pro/                        # Resultados de O3 Pro
```

### Estructura de Cada Run

Cada carpeta de modelo contiene m√∫ltiples ejecuciones con timestamp (`run_YYYYMMDD_HHMMSS/`):

```
run_20250704_102545/
‚îú‚îÄ‚îÄ config.yaml                # Snapshot de configuraci√≥n de la ejecuci√≥n
‚îú‚îÄ‚îÄ diagnoses.json            # DDX/GDX con c√≥digos m√©dicos por caso
‚îú‚îÄ‚îÄ ddx_analysis.csv          # An√°lisis de frecuencia de DDX generados
‚îú‚îÄ‚îÄ evaluation.log            # Logs detallados de ejecuci√≥n
‚îú‚îÄ‚îÄ run_summary.json          # M√©tricas globales y an√°lisis consolidado
‚îú‚îÄ‚îÄ semantic_evaluation.json  # Detalles de evaluaci√≥n sem√°ntica
‚îî‚îÄ‚îÄ severity_evaluation.json  # An√°lisis de severidad con m√©tricas de sesgo
```

## Flujo del Pipeline

### FASE 1: Procesamiento de Casos (Paralelo)

Para cada caso m√©dico:

1. **Generaci√≥n de DDX**: El LLM genera diagn√≥sticos diferenciales basados en la descripci√≥n del caso
2. **Extracci√≥n de C√≥digos**: Azure Text Analytics normaliza el texto y extrae c√≥digos m√©dicos (ICD10, SNOMED, ORPHA, OMIM)
3. **Evaluaci√≥n Sem√°ntica**: 
   - Compara DDX vs GDX usando jerarqu√≠a ICD10
   - Si el score ICD10 = 0.0, aplica BERT fallback:
     - Cruza todas las combinaciones de texto DDX-GDX
     - Acepta match si similaridad BERT ‚â• 0.80
4. **Evaluaci√≥n de Severidad**: Calcula distancias normalizadas entre severidades DDX-GDX

### FASE 2: Asignaci√≥n de Severidades

- Extrae diagn√≥sticos √∫nicos de todos los casos
- LLM asigna severidades (S0-S10) en lotes de 50
- Sistema de cach√© para reutilizar en evaluaciones futuras

**Escala de Severidad:**
- S0: M√≠nima/sin impacto
- S10: Cr√≠tica/potencialmente mortal

### FASE 3: Generaci√≥n de Archivos de Salida

## Archivos de Salida Detallados

### `run_summary.json`
Contiene m√©tricas globales del experimento:
- Configuraci√≥n utilizada
- M√©tricas de precisi√≥n (Top-1, Top-3, Top-5)
- An√°lisis por cap√≠tulo ICD10
- Estad√≠sticas de uso de BERT fallback
- Distribuci√≥n de tipos de c√≥digo (ICD10, SNOMED, BERT, none)
- Perfilado del dataset por origen (R, U, J, T, Q, B, S)

### `diagnoses.json`
Estructura JSON con todos los casos procesados:
- DDX generados con sus c√≥digos m√©dicos
- GDX de referencia con sus c√≥digos
- C√≥digos formateados en l√≠nea √∫nica para facilitar lectura

### `semantic_evaluation.json`
Evaluaci√≥n detallada caso por caso:
- Scores sem√°nticos para cada combinaci√≥n DDX-GDX
- Tipo de c√≥digo usado para matching (icd10/snomed/BERT/none)
- Mejor score alcanzado y diagn√≥sticos asociados

### `severity_evaluation.json`
An√°lisis de severidad incluyendo:
- Severidades asignadas a cada diagn√≥stico
- Distancias normalizadas DDX-GDX
- M√©tricas de sesgo:
  - Sesgo optimista (subestima severidad)
  - Sesgo pesimista (sobreestima severidad)

### `ddx_analysis.csv`
Tabla de frecuencia de diagn√≥sticos generados:
- Diagn√≥sticos m√°s comunes
- Frecuencia de aparici√≥n
- √ötil para detectar sesgos del modelo

## Comparaci√≥n de Modelos

Los archivos `model_comparison_*.txt` permiten comparar el rendimiento de m√∫ltiples modelos en un subconjunto de casos:

- **Sin BERT**: Evaluaci√≥n estricta usando solo c√≥digos m√©dicos
- **Con BERT**: Incluye fallback para casos sin matches exactos

Formato de comparaci√≥n:
```
CASO X/60: [ID]
----------------------------------------
### Modelo: [nombre]
Final Best Score: [score]
Associated DDX: [diagn√≥stico]
Associated GDX: [diagn√≥stico de referencia]
Code Type: [icd10/snomed/BERT/none]
```

## Configuraci√≥n (`config.yaml`)

### Par√°metros Principales:

```yaml
experiment_name: "Nombre del experimento"
dataset_path: "bench/datasets/all_450.json"  # Dataset creado con serve.py

dxgpt_emulator:
  model: "gpt-4o-summary"  # Opciones: o1, gpt-4o, medgemma, jonsnow, sakura, openbio, o3-pro, o3
  prompt_path: "ruta al prompt"
  params:
    temperature: 0.1
    max_tokens: 4000
    reasoning_effort: "high"  # Solo para o3/o3-pro: low, medium, high
  output_schema: true  # Fuerza formato de salida estructurado

severity_assigner:
  model: "gpt-4o-summary"
  prompt_path: "eval-prompts/severity_assignment_batch_prompt.txt"
```

## M√©tricas Clave

### Score Sem√°ntico
- M√°ximo score entre todas las comparaciones ICD10
- Si falla ICD10 (score = 0), aplica BERT con threshold ‚â• 0.80
- Registra `code_type` para trazabilidad

### Score de Severidad
- Promedio de distancias normalizadas: `|sev_ddx - sev_gdx| / max_distancia`
- Menor score = mejor alineaci√≥n de severidad

### Top-K Accuracy
- Porcentaje de casos con score ‚â• 0.8 en las primeras K posiciones
- M√©tricas para Top-1, Top-3, Top-5

### Cobertura de C√≥digos
- Distribuci√≥n de sistemas de codificaci√≥n utilizados
- Porcentaje de uso de BERT fallback
- Casos sin matches (code_type: none)

## Flujo de Trabajo Completo

```mermaid
graph LR
    A[9,677 casos m√©dicos] --> B[serve.py]
    B --> C[Dataset balanceado]
    C --> D[Pipeline v2]
    D --> E[Evaluaci√≥n con ICD10]
    E --> F{Score = 0?}
    F -->|S√≠| G[BERT fallback]
    F -->|No| H[Resultado final]
    G --> H
    H --> I[M√©tricas y reportes]
```

## Ejecuci√≥n

```bash
# Paso 1: Crear dataset de evaluaci√≥n (opcional si ya existe)
cd data29/data-repos/post-normalization\ \(server\)/
python serve.py --size 450

# Paso 2: Ejecutar pipeline de evaluaci√≥n
cd bench/pipelines/pipeline_v2\ -\ icd10\ +\ bert/
python run.py
```

El pipeline:
1. Carga la configuraci√≥n desde `config.yaml`
2. Crea directorio de resultados con timestamp
3. Procesa casos en paralelo
4. Genera todos los archivos de salida
5. Imprime resumen de m√©tricas en consola

## Modelos Soportados

- **gpt-4o / gpt-4o-summary**: GPT-4 optimizado
- **gpt-4.1**: Versi√≥n actualizada de GPT-4
- **o1**: Modelo O1 de OpenAI
- **o3 / o3-pro**: Modelos O3 con soporte para `reasoning_effort`
- **o3_images**: O3 con capacidad de procesamiento de im√°genes
- **medgemma**: Modelo m√©dico especializado
- **jonsnow, sakura, openbio**: Modelos alternativos para comparaci√≥n

## An√°lisis de Resultados

El pipeline facilita an√°lisis comparativos mediante:
- M√©tricas consistentes entre modelos
- Trazabilidad completa de decisiones (qu√© c√≥digo/m√©todo se us√≥)
- Detecci√≥n de sesgos de severidad
- Identificaci√≥n de fortalezas/debilidades por tipo de caso

Los archivos de comparaci√≥n de modelos (`model_comparison_*.txt`) proporcionan una vista detallada caso por caso para an√°lisis profundo del comportamiento de cada modelo.

## üîó Referencias y Recursos

- [Documentaci√≥n completa de datasets](../../../data29/README.md)
- [Script serve.py](../../../data29/data-repos/post-normalization%20(server)/README.md)
- [ICD-10 Browser](https://icd.who.int/browse10/2019/en)
- [BERT para similitud sem√°ntica](https://huggingface.co/sentence-transformers)