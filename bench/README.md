# Bench - Sistema de EvaluaciÃ³n de Modelos de DiagnÃ³stico ğŸ†

El directorio `bench` contiene el sistema de benchmarking para evaluar modelos de IA mÃ©dica en tareas de diagnÃ³stico diferencial, comparando su rendimiento con diagnÃ³sticos de referencia validados.

## ğŸ¯ PropÃ³sito

Evaluar sistemÃ¡ticamente la capacidad diagnÃ³stica de modelos LLM mediante:

1. **PrecisiÃ³n SemÃ¡ntica**: QuÃ© tan bien identifican el diagnÃ³stico correcto
2. **EvaluaciÃ³n de Severidad**: QuÃ© tan precisamente estiman la gravedad clÃ­nica

## ğŸ—ï¸ Arquitectura del Sistema

```
bench/
â”œâ”€â”€ README.md                           # Este archivo
â”œâ”€â”€ candidate-prompts/                  # Prompts para generar diagnÃ³sticos
â”‚   â”œâ”€â”€ candidate_output_schema.json    # Esquema de respuesta esperada
â”‚   â””â”€â”€ [varios prompts].txt            # Diferentes estrategias de prompting
â”œâ”€â”€ datasets/                           # Datasets mÃ©dicos evaluables
â”‚   â”œâ”€â”€ all_150.json                   # Dataset balanceado (150 casos)
â”‚   â”œâ”€â”€ all_250.json                   # Dataset medio (250 casos)
â”‚   â”œâ”€â”€ all_450.json                   # Dataset completo (450 casos)
â”‚   â””â”€â”€ [otros datasets].json          # Datasets especializados
â””â”€â”€ pipelines/                         # Pipelines de evaluaciÃ³n (3 versiones)
    â”œâ”€â”€ pipeline_v1 - icd10/           # Pipeline base con ICD-10
    â”œâ”€â”€ pipeline_v2 - icd10 + bert/   # Pipeline mejorada con BERT
    â””â”€â”€ pipeline_v3 - full LLM/       # Pipeline experimental LLM puro
```

## ğŸ”„ Flujo de EvaluaciÃ³n

El proceso completo sigue estos pasos:

### 1. GeneraciÃ³n de DiagnÃ³sticos (DDX)
```python
# El modelo recibe un caso clÃ­nico
caso = "Paciente de 45 aÃ±os con dolor torÃ¡cico..."

# Genera 5 diagnÃ³sticos diferenciales
ddx = ["Infarto", "Angina", "Reflujo", "Ansiedad", "Costocondritis"]
```

### 2. EvaluaciÃ³n SemÃ¡ntica
```python
# Comparamos DDX con diagnÃ³sticos correctos (GDX)
gdx = ["Infarto agudo de miocardio", "SÃ­ndrome coronario agudo"]

# SapBERT calcula similitud semÃ¡ntica
scores = {
    "Infarto": {"Infarto agudo de miocardio": 0.95},
    "Angina": {"SÃ­ndrome coronario agudo": 0.78},
    # ...
}
```

### 3. AsignaciÃ³n de Severidad
```python
# Un LLM asigna severidad a cada diagnÃ³stico Ãºnico
severidades = {
    "Infarto": "S9",        # Muy grave
    "Angina": "S7",         # Grave
    "Reflujo": "S3",        # Leve
    "Ansiedad": "S2",       # Muy leve
    "Costocondritis": "S1"  # MÃ­nima
}
```

### 4. CÃ¡lculo de MÃ©tricas
```python
# Score semÃ¡ntico: mejor match entre DDX y GDX
semantic_score = 0.95  # (Infarto â†” Infarto agudo)

# Score de severidad: distancia normalizada
severity_score = 0.15  # (cercano a severidad correcta)
```

## ğŸ“ Componentes Principales

### datasets/
Contiene los datasets mÃ©dicos procesados y validados para evaluaciÃ³n. Incluye datasets de diferentes tamaÃ±os y caracterÃ­sticas, todos con casos clÃ­nicos estructurados y sus diagnÃ³sticos de referencia (GDX).

### pipelines/
Sistema evolutivo de evaluaciÃ³n con tres pipelines que representan un avance metodolÃ³gico progresivo:

- **Pipeline v1 - ICD10**: EvaluaciÃ³n base usando clasificaciÃ³n ICD-10 directa
- **Pipeline v2 - ICD10 + BERT**: EvaluaciÃ³n mejorada incorporando embeddings biomÃ©dicos para mayor precisiÃ³n semÃ¡ntica
- **Pipeline v3 - Full LLM**: EvaluaciÃ³n experimental usando LLMs para todo el proceso sin embeddings pre-entrenados

Cada pipeline contiene su propio `config.yaml` para configuraciÃ³n de experimentos y carpeta `results/` para almacenar resultados.

### candidate-prompts/
Contiene los prompts e instrucciones para que los modelos generen diagnÃ³sticos diferenciales, incluyendo el esquema de respuesta esperado y mÃºltiples variantes de estrategias de prompting.

## ğŸš€ Ejecutar un Experimento

## ğŸ“š DocumentaciÃ³n Adicional

Para comprender el modelo conceptual del benchmarking y las notas de investigaciÃ³n detalladas, consulte:

ğŸ“ **`__benchmarking-conceptual-model-and-research-notes/`**
- Contiene anÃ¡lisis detallados, comparaciones entre pipelines y hallazgos de investigaciÃ³n
- Incluye visualizaciones y documentaciÃ³n del proceso evolutivo del sistema

### 1. Configurar el experimento

Editar `pipelines/[pipeline_version]/config.yaml`:
```yaml
experiment_name: "Mi Experimento GPT-4"
dataset_path: "bench/datasets/ramedis-45.json"

llm_configs:
  candidate_dx_gpt:
    model: "gpt-4o"  # o "jonsnow", "medgemma", etc.
    prompt: "../candidate-prompts/candidate_prompt.txt"
    
  severity_assigner_llm:
    model: "gpt-4o"
    prompt: "eval-prompts/severity_assignment_batch_prompt.txt"
```

## ğŸš¨ Consideraciones Importantes

- Los datasets son sintÃ©ticos/anonimizados, NO contienen datos reales de pacientes
- Los resultados son para investigaciÃ³n, NO para diagnÃ³stico clÃ­nico real
- La evaluaciÃ³n es automÃ¡tica, puede tener sesgos o limitaciones
- Siempre validar con expertos mÃ©dicos antes de conclusiones

## ğŸ”— Referencias

- [Pipeline v1 - DocumentaciÃ³n](pipelines/pipeline_v1%20-%20icd10/README.md)
- [Pipeline v2 - DocumentaciÃ³n](pipelines/pipeline_v2%20-%20icd10%20+%20bert/pipeline_README.md)
- [Modelo Conceptual - InvestigaciÃ³n](__benchmarking-conceptual-model-and-research-notes/)
- [Datasets - Origen y estructura](../data29/README.md)